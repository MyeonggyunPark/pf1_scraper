from bs4 import BeautifulSoup
import requests


class JobsScraper:
    def __init__(self):
        self.BASE_URL = ""
        self.USER_HEADERS = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"
        }
        self.infos_list = []

    def get_url(self, keyword):
        """keywordì— ë”°ë¼ URL ìƒì„±"""
        url = f"{self.BASE_URL}{keyword}/"
        return url
        

    def extract_text(self, tag):
        """ë‹¨ì¼ íƒœê·¸ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

        if isinstance(tag, list):
            return [t.text.strip() for t in tag] if tag else ["No Information"]
        elif tag:
            return tag.text.strip()
        else:
            return "No Information"

    def get_pages(self, url):
        """í•´ë‹¹ URLì˜ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸"""

        print(f"ğŸ“„ Checking pages for: {url}")
        response = requests.get(url, headers=self.USER_HEADERS)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            page_list = soup.find_all("a", class_="page-numbers")
            return len(page_list)
        else:
            print(f"âŒ Error fetching pages: {response.status_code}")
            return 0

    def get_infos(self, url):
        """ê° URLì—ì„œ ê³µê³  ì •ë³´ ì¶”ì¶œ"""

        print(f"ğŸ” Scraping: {url}")
        response = requests.get(url, headers=self.USER_HEADERS)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            jobs_list = soup.find_all("li", class_="bjs-jlid")

            for job in jobs_list:
                title_tag = job.find("h4", class_="bjs-jlid__h")
                company_tag = job.find("a", class_="bjs-jlid__b")
                skills_tag = job.find_all("a", class_="bjs-bl bjs-bl-porcelain")
                description_tag = job.find("div", class_="bjs-jlid__description")

                link_tag = title_tag.find("a") if title_tag else None
                title_link = (
                    link_tag["href"]
                    if link_tag and "href" in link_tag.attrs
                    else "No Link"
                )

                infos = {
                    "company": self.extract_text(company_tag),
                    "title": self.extract_text(title_tag),
                    "skills": self.extract_text(skills_tag),
                    "description": self.extract_text(description_tag),
                    "link": title_link,
                }

                self.infos_list.append(infos)
        else:
            print(f"âŒ Error fetching infos: {response.status_code}")

    def infos_print(self, info):
        """ì •ë³´ ì¶œë ¥ í•¨ìˆ˜"""
        print("\n======= [ğŸ“‘ INFO] ======")
        for k, v in info.items():
            if isinstance(v, list):
                print(f"{k} {' / '.join(v)}")
            else:
                print(f"{k} {v}")

    def run(self):
        """ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜"""

        # URL ëª©ë¡ ìƒì„±
        for keyword in self.all_keywords:
            self.get_url(keyword)

        # ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ engineeringì˜ í˜ì´ì§€ ìˆ˜ë§Œí¼ ìˆœíšŒ
        first_url = self.urls_list[0]
        total_pages = self.get_pages(first_url)

        for page in range(total_pages):
            paged_url = f"{first_url}page/{page + 1}"
            self.get_infos(paged_url)

        # skill-based URLë“¤ ì¶”ì¶œ
        for url in self.urls_list[2:]:
            self.get_infos(url)

        # ê²°ê³¼ ì¶œë ¥
        for info in self.infos_list:
            self.infos_print(info)


if __name__ == "__main__":
    scraper = JobsScraper()
    scraper.run()
